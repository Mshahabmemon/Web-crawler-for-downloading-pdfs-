{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0deeb8-90bc-456c-bcea-bc6b0538b82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\shaha\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b57543ce-fedc-48ea-84a4-1af546e5d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# BLOCK 1 ‚Äî Imports & Setup\n",
    "# ==========================\n",
    "import os, re, time, json, logging, hashlib\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ---- Exa AI config (set env: EXA_API_KEY) ----\n",
    "EXA_API_KEY = os.getenv(\"EXA_API_KEY\", \"d906a649-ab82-457c-9ece-3ae8d581d7a7\").strip()\n",
    "EXA_ENDPOINT = \"https://api.exa.ai/search\"\n",
    "\n",
    "# ---- Logging ----\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "# ---- HTTP session with retries/backoff ----\n",
    "USER_AGENT = \"PCF-Harvester/3.0 (+https://yourproject.example)\"\n",
    "REQUEST_TIMEOUT = 30\n",
    "THROTTLE_SEC = 0.8\n",
    "\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"User-Agent\": USER_AGENT,\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "})\n",
    "\n",
    "retry = Retry(\n",
    "    total=6, connect=3, read=3,\n",
    "    backoff_factor=0.8,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"HEAD\", \"GET\", \"POST\", \"OPTIONS\"]\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry, pool_connections=50, pool_maxsize=50)\n",
    "SESSION.mount(\"https://\", adapter)\n",
    "SESSION.mount(\"http://\", adapter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c1fcbc2-6023-4a75-af46-ff18d4b14d6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m18\u001b[0m\n\u001b[1;33m    except Exception as e2:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# BLOCK 2 ‚Äî Utilities\n",
    "# ==========================\n",
    "def get_html(url: str, timeout: int = REQUEST_TIMEOUT) -> Optional[BeautifulSoup]:\n",
    "    try:\n",
    "        r = SESSION.get(url, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "        return BeautifulSoup(r.text, \"lxml\")\n",
    "    except Exception as e1:\n",
    "        logging.warning(\"GET primary failed for %s: %s\", url, e1)\n",
    "        # second try with Referer\n",
    "        try:\n",
    "from urllib.parse import urlparse\n",
    "ref = {\"Referer\": f\"{urlparse(url).scheme}://{urlparse(url).netloc}/\"}\n",
    "            r2 = SESSION.get(url, headers=ref, timeout=timeout)\n",
    "            r2.raise_for_status()\n",
    "            return BeautifulSoup(r2.text, \"lxml\")\n",
    "        except Exception as e2:\n",
    "            logging.error(\"GET failed for %s: %s\", url, e2)\n",
    "            return None\n",
    "\n",
    "def fetch_html(url: str, timeout: int = REQUEST_TIMEOUT) -> Optional[str]:\n",
    "    try:\n",
    "        r = SESSION.get(url, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "    except Exception as e:\n",
    "        logging.warning(\"GET %s failed: %s\", url, e)\n",
    "        return None\n",
    "\n",
    "def is_pdf_url(url: str) -> bool:\n",
    "    u = url.lower()\n",
    "    return u.endswith(\".pdf\") or \".pdf?\" in u\n",
    "\n",
    "def etld(url: str) -> str:\n",
    "    try:\n",
    "        return urlparse(url).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def sha256_bytes(b: bytes) -> str:\n",
    "    h = hashlib.sha256(); h.update(b); return h.hexdigest()\n",
    "\n",
    "def product_type_tokens(ptype: str) -> List[str]:\n",
    "    p = (ptype or \"\").strip().lower()\n",
    "    tokens = []\n",
    "\n",
    "    if not p:\n",
    "        return []\n",
    "\n",
    "    if \"laptop\" in p or \"notebook\" in p:\n",
    "        # Accept any string that ends with \"book\", like MacBook, Chromebook, Ultrabook\n",
    "        tokens.extend([\"laptop\", \"notebook\", \"chromebook\", \"macbook\", \"ultrabook\", \"book\"])\n",
    "    elif \"desktop\" in p or \"pc\" in p:\n",
    "        tokens.extend([\"desktop\", \"pc\", \"tower\", \"mini\"])\n",
    "    elif \"monitor\" in p or \"display\" in p:\n",
    "        tokens.extend([\"monitor\", \"display\"])\n",
    "    elif \"server\" in p:\n",
    "        tokens.extend([\"server\"])\n",
    "    else:\n",
    "        tokens.append(p)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def merge_pdf_lists(*lists: List[Dict[str, str]]) -> List[Dict[str, str]]:\n",
    "    seen, merged = set(), []\n",
    "    for lst in lists:\n",
    "        for p in lst:\n",
    "            u = p.get(\"url\")\n",
    "            if not u or u in seen: \n",
    "                continue\n",
    "            seen.add(u); merged.append(p)\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5eba5f-6254-488f-b6e5-a4bf7418d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# BLOCK 3 ‚Äî Exa AI client + fallback\n",
    "# ==========================\n",
    "def exa_search(query: str, top_k: int = 20) -> List[Dict[str, Any]]:\n",
    "    if not EXA_API_KEY:\n",
    "        logging.error(\"EXA_API_KEY not set. Exa search disabled.\")\n",
    "        return []\n",
    "    headers = {\n",
    "        \"x-api-key\": EXA_API_KEY,\n",
    "        \"User-Agent\": USER_AGENT,\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "    payload = {\"query\": query, \"numResults\": top_k}\n",
    "    try:\n",
    "        r = SESSION.post(EXA_ENDPOINT, json=payload, headers=headers, timeout=REQUEST_TIMEOUT)\n",
    "        r.raise_for_status()\n",
    "        data = r.json() or {}\n",
    "        return data.get(\"results\") or data.get(\"documents\") or []\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exa API error: %s\", e)\n",
    "        return []\n",
    "\n",
    "PCF_PDF_KEYWORDS = [\n",
    "    '\"product carbon footprint\"', '\"Product Environmental Report\"',\n",
    "    '\"life cycle assessment\"', 'LCA', 'EPD', '\"kg CO2\"', '\"kg CO2e\"'\n",
    "]\n",
    "\n",
    "def harvest_pcf_pdfs_via_search(brand: str, product_type: str, brand_domain: str,\n",
    "                                top_k_per_query: int = 30) -> List[Dict[str, str]]:\n",
    "    syns = product_type_synonyms(product_type)\n",
    "    syn_clause = \"(\" + \" OR \".join(f'\"{s}\"' for s in syns) + \")\" if syns else \"\"\n",
    "    queries: List[str] = []\n",
    "    for kw in PCF_PDF_KEYWORDS:\n",
    "        queries.append(f'site:{brand_domain} filetype:pdf {brand} {kw}')\n",
    "        if syn_clause:\n",
    "            queries.append(f'site:{brand_domain} filetype:pdf {brand} {syn_clause} {kw}')\n",
    "\n",
    "    seen, out = set(), []\n",
    "    for q in queries:\n",
    "        logging.info(\"Exa fallback query: %s\", q)\n",
    "        hits = exa_search(q, top_k=top_k_per_query)\n",
    "        for h in hits:\n",
    "            url = (h.get(\"url\") or \"\").strip()\n",
    "            title = (h.get(\"title\") or \"\").strip()\n",
    "            if not url or not url.lower().endswith(\".pdf\"):\n",
    "                continue\n",
    "            if brand_domain not in etld(url):\n",
    "                continue\n",
    "            if url in seen:\n",
    "                continue\n",
    "            seen.add(url)\n",
    "            out.append({\"url\": url, \"product_text\": title or url.rsplit(\"/\", 1)[-1]})\n",
    "        time.sleep(THROTTLE_SEC)\n",
    "    logging.info(\"Exa fallback harvested %d PDF(s) for %s/%s\", len(out), brand, product_type)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b40c695-f58d-4818-a52a-92270e3fb81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# BLOCK 4 ‚Äî Landing navigation\n",
    "# ==========================\n",
    "#def follow_view_pcfs(landing_url: str) -> str:\n",
    " #   soup = get_html(landing_url)\n",
    "  #  if not soup:\n",
    "       # return landing_url\n",
    "  #  cta_needles = [\n",
    "     #   \"view pcfs\", \"see pcfs\", \"product carbon footprints\",\n",
    "      #  \"product environmental report\", \"environmental report\",\n",
    "       # \"view all\", \"see all\"\n",
    "   # ]\n",
    "    #for a in soup.find_all(\"a\", href=True):\n",
    "     #   txt = (a.get_text(\" \") or \"\").strip().lower()\n",
    "      #  if any(k in txt for k in cta_needles):\n",
    "       #     return urljoin(landing_url, a[\"href\"])\n",
    "    #return landing_url\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def _score_cta_link(base_url: str, a) -> int:\n",
    "    href = a.get(\"href\", \"\")\n",
    "    txt = (a.get_text(\" \") or \"\").strip().lower()\n",
    "    if not href:\n",
    "        return -999\n",
    "\n",
    "    abs_url = urljoin(base_url, href)\n",
    "    s = f\"{txt} {abs_url}\".lower()\n",
    "\n",
    "    # ‚ùå Kill immediately if irrelevant section\n",
    "    bad = [\"support\", \"services\", \"drivers\", \"partners\", \"marketing\", \"blog\", \"press\", \"solution\", \"contact\"]\n",
    "    if any(b in s for b in bad):\n",
    "        return -999\n",
    "\n",
    "    # üéØ Jackpot: direct PDF with strong indicators\n",
    "    if href.lower().endswith(\".pdf\") and any(k in s for k in [\"pcf\", \"carbon\", \"footprint\", \"lca\", \"epd\"]):\n",
    "        return 999  # stop right there, valid report found\n",
    "\n",
    "    score = 0\n",
    "\n",
    "    # ‚úÖ Strong PCF-related content\n",
    "    if any(t in s for t in [\n",
    "        \"product-carbon\", \"carbon-footprint\", \"product carbon footprint\",\n",
    "        \"environmental\", \"product environmental report\", \"epd\", \"sustainab\", \"pcf\"\n",
    "    ]):\n",
    "        score += 10\n",
    "\n",
    "    # üí° Bonus: product-specific mentions\n",
    "# Match product types and any '...book' variants (e.g., MacBook, Notebook, Chromebook)\n",
    "    if any(p in s for p in [\"laptop\", \"desktop\", \"monitor\", \"server\", \"device\"]) or \"book\" in s:\n",
    "       score += 3\n",
    "\n",
    "\n",
    "    # üîí Domain and structure boost\n",
    "    base = urlparse(base_url)\n",
    "    p = urlparse(abs_url)\n",
    "    if p.netloc != base.netloc:\n",
    "        score -= 5\n",
    "    if abs_url.startswith(base_url.rstrip(\"/\")):\n",
    "        score += 2\n",
    "\n",
    "    if \"#\" in href:\n",
    "        score += 1\n",
    "\n",
    "    return score\n",
    "\n",
    "def follow_view_pcfs(landing_url: str) -> str:\n",
    "    soup = get_html(landing_url)\n",
    "    if not soup:\n",
    "        return landing_url\n",
    "\n",
    "    best_url, best_score = None, -10**9\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        sc = _score_cta_link(landing_url, a)\n",
    "        if sc > best_score:\n",
    "            best_score = sc\n",
    "            best_url = urljoin(landing_url, a[\"href\"])\n",
    "\n",
    "    # Only follow if it looks *strongly* like a PCF hub; else stay put\n",
    "    if best_url and best_score >= 8:\n",
    "        logging.info(f\"follow_view_pcfs: following best link (score {best_score}) ‚Üí {best_url}\")\n",
    "        return best_url\n",
    "\n",
    "    logging.info(\"follow_view_pcfs: staying on landing page (no strong PCF CTA found)\")\n",
    "    return landing_url\n",
    "\n",
    "def _norm_type(s: str) -> str:\n",
    "    return (s or \"\").strip().lower().rstrip(\"s\")\n",
    "\n",
    "def resolve_product_tab(pcfs_url: str, product_type: str) -> str:\n",
    "    soup = get_html(pcfs_url)\n",
    "    if not soup:\n",
    "        return pcfs_url\n",
    "\n",
    "    wanted = _norm_type(product_type)\n",
    "    base = pcfs_url.split(\"#\")[0]\n",
    "\n",
    "    # A) Exact tab anchor by text\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        label = _norm_type(a.get_text(\" \"))\n",
    "        if label == wanted:\n",
    "            href = a[\"href\"]\n",
    "            # accept only same-page anchors or same-family URLs\n",
    "            if href.startswith(\"#\"):\n",
    "                return base + href\n",
    "            absu = urljoin(pcfs_url, href)\n",
    "            if absu.startswith(base):  # same page family\n",
    "                return absu\n",
    "\n",
    "    # B) aria-controls / data-bs-target panel\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        label = _norm_type(a.get_text(\" \"))\n",
    "        if label != wanted:\n",
    "            continue\n",
    "        panel_id = a.get(\"aria-controls\") or a.get(\"data-bs-target\") or \"\"\n",
    "        if panel_id.startswith(\"#\"): \n",
    "            panel_id = panel_id[1:]\n",
    "        if panel_id:\n",
    "            return f\"{base}#{panel_id}\"\n",
    "\n",
    "    # C) headings with ids\n",
    "    for h in soup.find_all([\"h2\", \"h3\", \"h4\"]):\n",
    "        if _norm_type(h.get_text(\" \")) == wanted and h.get(\"id\"):\n",
    "            return f\"{base}#{h['id']}\"\n",
    "\n",
    "    # If nothing reliable found, do NOT jump to some other random page\n",
    "    logging.info(\"resolve_product_tab: no product tab found, staying on PCF hub\")\n",
    "    return pcfs_url\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62719c80-3eb6-4158-bb08-fb99fc9a8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# BLOCK 5 ‚Äî Robust per-page PDF extraction\n",
    "# ==========================\n",
    "PDF_REGEX = re.compile(r'https?://[^\\s\"\\'<>]+\\.pdf(?:\\?[^\\s\"\\'<>]*)?', re.I)\n",
    "\n",
    "def _collect_pdf_links_from_html(url: str, html: str) -> List[Dict[str, str]]:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    found: List[Dict[str, str]] = []\n",
    "\n",
    "    # 1) <a href=\"...pdf\">\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if \".pdf\" in href.lower():\n",
    "            found.append({\"url\": urljoin(url, href), \"product_text\": (a.get_text(\" \") or \"\").strip()})\n",
    "\n",
    "    # 2) Common data-* attributes\n",
    "    ATTRS = [\"data-href\",\"data-url\",\"data-download\",\"data-asset-url\"]\n",
    "    for tag in soup.find_all(True):\n",
    "        for attr in ATTRS:\n",
    "            val = tag.get(attr)\n",
    "            if val and \".pdf\" in val.lower():\n",
    "                found.append({\"url\": urljoin(url, val), \"product_text\": (tag.get_text(\" \") or \"\").strip()})\n",
    "\n",
    "    # 3) Regex sweep across raw HTML (captures inline JSON/script)\n",
    "    for m in PDF_REGEX.finditer(html):\n",
    "        found.append({\"url\": urljoin(url, m.group(0)), \"product_text\": \"\"})\n",
    "\n",
    "    # Dedupe\n",
    "    seen, out = set(), []\n",
    "    for x in found:\n",
    "        u = x[\"url\"]\n",
    "        if u in seen: \n",
    "            continue\n",
    "        seen.add(u); out.append(x)\n",
    "    return out\n",
    "\n",
    "def extract_pdfs_page_robust(page_url: str, require_tokens: Optional[List[str]] = None) -> List[Dict[str, str]]:\n",
    "    html = fetch_html(page_url, REQUEST_TIMEOUT)\n",
    "    if html is None:\n",
    "        return []\n",
    "    pdfs = _collect_pdf_links_from_html(page_url, html)\n",
    "    if require_tokens:\n",
    "        toks = [t.lower() for t in require_tokens]\n",
    "        keep: List[Dict[str, str]] = []\n",
    "        for p in pdfs:\n",
    "            ctx = (p.get(\"product_text\") or \"\").lower()\n",
    "            if any(t in ctx for t in toks) or re.search(r\"\\\\b\\\\w*book\\\\b\", ctx): # <- match \"MacBook\", \"Chromebook\", etc.\n",
    "                keep.append(p)\n",
    "            logging.info(f\"Filtered PDFs using tokens + 'book' logic ‚Üí kept {len(keep)} of {len(pdfs)}\")\n",
    "            return keep\n",
    "    return pdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c00b01-9fa2-4e3f-80c0-1e2193212e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#block 5.5\n",
    "def extract_model_pdfs_by_section(soup: BeautifulSoup, product_tokens: List[str]) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Grouped extraction: find PDFs under sections likely referring to the product type.\n",
    "    Uses headings (h2/h3), list items, and anchor tags.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    current_section = None\n",
    "    section_map = {}\n",
    "\n",
    "    # Normalize token list for matching\n",
    "    tokens = [t.lower() for t in product_tokens]\n",
    "\n",
    "    for tag in soup.find_all([\"h2\", \"h3\", \"li\", \"a\"]):\n",
    "        text = (tag.get_text(\" \") or \"\").strip().lower()\n",
    "\n",
    "        if tag.name in [\"h2\", \"h3\"]:\n",
    "            # Start of a new section\n",
    "            current_section = text\n",
    "        elif tag.name == \"li\" and any(tok in text for tok in tokens):\n",
    "            # List items that may contain model names\n",
    "            current_section = text\n",
    "        elif tag.name == \"a\" and tag.has_attr(\"href\") and tag[\"href\"].endswith(\".pdf\"):\n",
    "            href = urljoin(soup.base_url or \"\", tag[\"href\"])\n",
    "            if current_section and any(tok in current_section for tok in tokens):\n",
    "                results.append({\n",
    "                    \"url\": href,\n",
    "                    \"product_text\": tag.get_text(\" \").strip() or current_section\n",
    "                })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70705b42-34b9-49a2-90f2-769357a5730b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# BLOCK 6 ‚Äî Same-domain BFS (depth 2)\n",
    "# ==========================\n",
    "from collections import deque\n",
    "\n",
    "def _same_domain(u: str, domain: str) -> bool:\n",
    "    return etld(u).endswith(domain.lower())\n",
    "\n",
    "def _normalize_link(base: str, href: str) -> Optional[str]:\n",
    "    if not href or href.startswith(\"javascript:\") or href.startswith(\"mailto:\"):\n",
    "        return None\n",
    "    return urljoin(base, href)\n",
    "\n",
    "def is_product_pdf(url: str, product_tokens: Optional[List[str]] = None) -> bool:\n",
    "    \"\"\"\n",
    "    Match PDF filenames to known product tokens or patterns like *book.\n",
    "    \"\"\"\n",
    "    if not url.lower().endswith(\".pdf\"):\n",
    "        return False\n",
    "    name = url.lower()\n",
    "\n",
    "    # Regex match: anything ending in 'book' (e.g., MacBook, Notebook, Ultrabook)\n",
    "    if re.search(r\"\\b\\w*book\\b\", name):\n",
    "        return True\n",
    "\n",
    "    if product_tokens:\n",
    "        return any(tok in name for tok in product_tokens)\n",
    "    \n",
    "    return True  # fallback if no tokens provided\n",
    "\n",
    "def bfs_collect_pdfs(start_url: str,\n",
    "                     domain: str,\n",
    "                     max_pages: int = 60,\n",
    "                     max_depth: int = 2,\n",
    "                     per_page_sleep: float = 0.4,\n",
    "                     require_tokens: Optional[List[str]] = None) -> List[Dict[str, str]]:\n",
    "    visited, q = set(), deque([(start_url, 0)])\n",
    "    all_pdfs: List[Dict[str, str]] = []\n",
    "    pages_seen = 0\n",
    "\n",
    "    while q and pages_seen < max_pages:\n",
    "        url, depth = q.popleft()\n",
    "        if url in visited:\n",
    "            continue\n",
    "        visited.add(url)\n",
    "        pages_seen += 1\n",
    "\n",
    "        html = fetch_html(url, REQUEST_TIMEOUT)\n",
    "        if html is None:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "        # ---- Extract all PDF links ----\n",
    "        page_pdfs = []\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = _normalize_link(url, a[\"href\"])\n",
    "            if not href or not is_pdf_url(href):\n",
    "                continue\n",
    "            if not _same_domain(href, domain):\n",
    "                continue\n",
    "            if is_product_pdf(href, product_tokens=require_tokens):\n",
    "                page_pdfs.append({\"url\": href, \"source\": url})\n",
    "\n",
    "        all_pdfs = merge_pdf_lists(all_pdfs, page_pdfs)\n",
    "\n",
    "        # ---- Queue next HTML pages ----\n",
    "        if depth >= max_depth:\n",
    "            continue\n",
    "\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = _normalize_link(url, a[\"href\"])\n",
    "            if not href or href.lower().endswith(\".pdf\"):\n",
    "                continue\n",
    "            if not _same_domain(href, domain):\n",
    "                continue\n",
    "            q.append((href, depth + 1))\n",
    "\n",
    "        time.sleep(per_page_sleep)\n",
    "\n",
    "    return all_pdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba8d37-0767-40a6-bdf3-b44f1377cf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# BLOCK 7 ‚Äî Download PDFs\n",
    "# ==========================\n",
    "def download_all(pdfs: List[Dict[str, str]], brand: str, out_dir: str = \"data/pcf\") -> List[Dict[str, Any]]:\n",
    "    os.makedirs(os.path.join(out_dir, brand.lower()), exist_ok=True)\n",
    "    saved: List[Dict[str, Any]] = []\n",
    "    for i, p in enumerate(pdfs, 1):\n",
    "        url = p[\"url\"]\n",
    "        try:\n",
    "            head = SESSION.head(url, allow_redirects=True, timeout=REQUEST_TIMEOUT)\n",
    "            ct = (head.headers.get(\"Content-Type\") or \"\").lower()\n",
    "            if \"pdf\" not in ct and not is_pdf_url(url):\n",
    "                logging.info(\"Skip non-PDF: %s (ct=%s)\", url, ct)\n",
    "                continue\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            r = SESSION.get(url, timeout=REQUEST_TIMEOUT)\n",
    "            r.raise_for_status()\n",
    "            blob = r.content\n",
    "            digest = sha256_bytes(blob)[:16]\n",
    "            fname = f\"{digest}.pdf\"\n",
    "            fpath = os.path.join(out_dir, brand.lower(), fname)\n",
    "            with open(fpath, \"wb\") as f:\n",
    "                f.write(blob)\n",
    "            saved.append({\n",
    "                \"url\": url,\n",
    "                \"file\": fpath,\n",
    "                \"bytes\": len(blob),\n",
    "                \"product_text\": p.get(\"product_text\",\"\")\n",
    "            })\n",
    "            logging.info(\"[%d/%d] Saved %s\", i, len(pdfs), fpath)\n",
    "            time.sleep(0.3)\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Download failed %s: %s\", url, e)\n",
    "    return saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be14655-f4a0-4cb1-b25b-1565b318340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# BLOCK 8 ‚Äî Orchestrator\n",
    "# ==========================\n",
    "def run_brand_producttype(\n",
    "    brand: str,\n",
    "    product_type: str,\n",
    "    landing_url: str,\n",
    "    pcfs_url: str,\n",
    "    is_pdf_listing_page: bool = False,\n",
    "    min_expected: int = 20\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    1) Follow landing ‚Üí PCFs hub.\n",
    "    2) If page already lists PDFs, extract directly.\n",
    "    3) Otherwise, resolve product tab and crawl.\n",
    "    4) Fallback to Exa search if needed.\n",
    "    \"\"\"\n",
    "    brand = brand.strip()\n",
    "    product_type = product_type.strip()\n",
    "    domain = etld(landing_url)\n",
    "\n",
    "    # ‚ö†Ô∏è Case 1: landing is itself a direct PDF\n",
    "    if pcfs_url.lower().endswith(\".pdf\"):\n",
    "        logging.info(\"Landing page is a direct PDF ‚Äî skipping crawl.\")\n",
    "        saved = download_all(\n",
    "            [{\"url\": pcfs_url, \"product_text\": os.path.basename(pcfs_url)}],\n",
    "            brand=brand\n",
    "        )\n",
    "        return {\n",
    "            \"brand\": brand,\n",
    "            \"product_type\": product_type,\n",
    "            \"landing_url\": landing_url,\n",
    "            \"pcfs_url\": pcfs_url,\n",
    "            \"tab_url\": pcfs_url,\n",
    "            \"found_count\": 1,\n",
    "            \"downloaded_count\": len(saved),\n",
    "            \"saved\": saved\n",
    "        }\n",
    "\n",
    "    # ‚ö†Ô∏è Case 2: user confirms the landing page itself contains all relevant PDFs\n",
    "    if is_pdf_listing_page:\n",
    "        logging.info(\"User confirmed this is the PDF listing page. Skipping tab + BFS.\")\n",
    "        page_pdfs = extract_pdfs_page_robust(pcfs_url, require_tokens=product_type_tokens(product_type))\n",
    "        crawled = []\n",
    "        tab_url = pcfs_url\n",
    "    else:\n",
    "        # ‚úÖ Standard flow: try to resolve product tab and crawl\n",
    "        tab_url = resolve_product_tab(pcfs_url, product_type)\n",
    "        time.sleep(THROTTLE_SEC)\n",
    "\n",
    "        html = fetch_html(tab_url)\n",
    "        if html:\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "            soup.base_url = tab_url\n",
    "            page_pdfs = extract_model_pdfs_by_section(soup, product_type_tokens(product_type))\n",
    "        else:\n",
    "            page_pdfs = []\n",
    "\n",
    "        crawled = bfs_collect_pdfs(\n",
    "            start_url=tab_url, domain=domain,\n",
    "            max_pages=60, max_depth=2, per_page_sleep=0.4,\n",
    "            require_tokens=product_type_tokens(product_type)\n",
    "        )\n",
    "\n",
    "    # ‚úÖ Merge page + crawl results\n",
    "    merged = merge_pdf_lists(page_pdfs, crawled)\n",
    "    logging.info(\"After HTML + BFS: %d PDF(s)\", len(merged))\n",
    "\n",
    "    # üîÅ Fallback to Exa if PDFs are insufficient\n",
    "    if len(merged) < min_expected:\n",
    "        logging.warning(\"Only %d PDFs found; switching to Exa fallback.\", len(merged))\n",
    "        exa_pdfs = harvest_pcf_pdfs_via_search(brand, product_type, domain, top_k_per_query=40)\n",
    "        merged = merge_pdf_lists(merged, exa_pdfs)\n",
    "        logging.info(\"After Exa merge: %d PDF(s)\", len(merged))\n",
    "\n",
    "    # üíæ Download results\n",
    "    saved = download_all(merged, brand=brand)\n",
    "\n",
    "    return {\n",
    "        \"brand\": brand,\n",
    "        \"product_type\": product_type,\n",
    "        \"landing_url\": landing_url,\n",
    "        \"pcfs_url\": pcfs_url,\n",
    "        \"tab_url\": tab_url,\n",
    "        \"found_count\": len(merged),\n",
    "        \"downloaded_count\": len(saved),\n",
    "        \"saved\": saved\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76dba0fe-f458-4ad0-8a52-3fa4893afc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# BLOCK 9 ‚Äî Landing Page Auto-Discovery\n",
    "# ==========================\n",
    "def find_landing_url(brand: str) -> str:\n",
    "    \"\"\"Use Exa AI to auto-discover the PCF landing page for a given brand.\"\"\"\n",
    "    logging.info(f\"Querying Exa for landing page of {brand} ...\")\n",
    "    payload = {\n",
    "        \"query\": f'site:{brand}.com (\"product carbon footprint\" OR \"Product Environmental Report\" OR sustainability)',\n",
    "        \"numResults\": 10\n",
    "    }\n",
    "    headers = {\"x-api-key\": EXA_API_KEY, \"User-Agent\": USER_AGENT}\n",
    "    try:\n",
    "        r = requests.post(EXA_ENDPOINT, json=payload, headers=headers, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        for item in data.get(\"results\", []):\n",
    "            url = item.get(\"url\", \"\")\n",
    "            if any(k in url.lower() for k in [\"footprint\", \"sustainab\", \"product-carbon\", \"epd\"]):\n",
    "                return url\n",
    "    except Exception as e:\n",
    "        logging.error(\"Exa error while finding landing page: %s\", e)\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0328e-ea1e-4bca-9c49-634e9c7b833e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# BLOCK 10 ‚Äî Main (brand-agnostic)\n",
    "# ==========================\n",
    "if __name__ == \"__main__\":\n",
    "    brand = input(\"Brand (e.g., dell, acer, hp): \").strip()\n",
    "    product_type = input(\"Product Type (e.g., Laptops): \").strip()\n",
    "    landing_url = input(\"PCF Landing URL (press Enter to auto-find): \").strip()\n",
    "\n",
    "    if not landing_url:\n",
    "        logging.info(f\"Searching Exa for {brand} PCF landing page...\")\n",
    "        landing_url = find_landing_url(brand)\n",
    "        if not landing_url:\n",
    "            raise SystemExit(f\"Could not auto-locate PCF page for {brand}.\")\n",
    "        logging.info(f\"Auto-detected landing page: {landing_url}\")\n",
    "   # Ask user if this page already contains all PDFs\n",
    "    use_as_is = input(\"Does this page ALREADY contain the PCF reports? (y/n): \").strip().lower()\n",
    "    pcfs_url = landing_url if use_as_is.startswith(\"y\") else follow_view_pcfs(landing_url)\n",
    "\n",
    "\n",
    "    # Dynamic threshold: if brand tends to have lots of PDFs, raise it\n",
    "    summary = run_brand_producttype(brand, product_type, landing_url,pcfs_url,is_pdf_listing_page=use_as_is.startswith(\"y\"), min_expected=20)\n",
    "\n",
    "    print(\"\\nSummary\")\n",
    "    print(\"-------\")\n",
    "    print(f\"Brand / Type:  {summary['brand']} / {summary['product_type']}\")\n",
    "    print(f\"Landing URL:   {summary['landing_url']}\")\n",
    "    print(f\"PCFs hub:      {summary['pcfs_url']}\")\n",
    "    print(f\"Tab URL:       {summary['tab_url']}\")\n",
    "    print(f\"Found PDFs:    {summary['found_count']}\")\n",
    "    print(f\"Downloaded:    {summary['downloaded_count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba233d-4965-4de5-8f6e-ba35c8501709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
